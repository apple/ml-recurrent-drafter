# Copyright Â© 2024 Apple Inc.
from dataclasses import dataclass
from typing import Optional, Tuple

import mlx.core as mx
import mlx.nn


@dataclass
class SamplingArgs:
    temperature: float
    greedy: bool  # greedy_search=True eliminates randomness


@dataclass
class SpecialTokens:
    pad: int
    eos: Optional[int] = 1


def _select_one_per_row(x: mx.array, batch_index: mx.array) -> mx.array:
    """x: (batch_size, seq_len, ....). batch_index: (batch_size,),int.
    Return (batch_size, 1, ...)."""
    return x[mx.arange(x.shape[0]), batch_index]


def _greedy_choose_from_candidates(
    beams_by_drafter: mx.array,
    beams_by_llm: mx.array,
) -> Tuple[mx.array, mx.array]:  # n_tokens_in_seq  # seq_in_beam
    """Choose a candidate token sequence from each of the batch of beams by drafter, and choose
    the first n tokens from that sequence. With greedy_search enabled, the drafter's longest
    candidate token sequence exact-matching the candidate token sequence by llm would be chosen.

      beams_by_drafter: (batch_size, beam_width, beam_length) Contains the init token generated
      by llm.

      beams_by_llm: (batch_size, beam_width, beam_length) Doesn't include the init token but
      contains an additional token sampled after beams_by_drafter.

      For example, for a candidate token sequence (beam_length=5)
      In beams_by_drafter:  xxxxx    # The first token is the init token generated by llm.
      In beams_by_llm:       xxxxx   # The last token is generated after beams_by_drafter by llm.

    Returns:

      n_tokens_in_seq: (batch_size) The number of candidate tokens chosen in the chosen seq_in_beam.

      seq_in_beam: (batch_size) The index of the chosen sequence in each of the batch of beams.
    """
    beams_by_drafter_without_init = beams_by_drafter[:, :, 1:]
    beams_by_llm_without_last = beams_by_llm[:, :, :-1]
    compare = (beams_by_drafter_without_init == beams_by_llm_without_last).astype(mx.int32)
    n_tokens_in_seqs = mx.sum(mx.cumprod(compare, axis=-1), axis=-1)
    kth = n_tokens_in_seqs.shape[1] - 1
    seq_in_beam = mx.argpartition(n_tokens_in_seqs, kth=kth, axis=-1)[:, kth:]
    n_tokens_in_seq = mx.take_along_axis(n_tokens_in_seqs, seq_in_beam, axis=-1)
    return n_tokens_in_seq.squeeze(axis=-1), seq_in_beam.squeeze(axis=-1)


def _choose_from_candidates(
    beams: mx.array,
    log_probs_by_llm: mx.array,
    log_probs_by_drafter: mx.array,
) -> Tuple[mx.array, mx.array]:  # n_tokens_in_seq  # seq_in_beam
    """Choose a candidate token sequence from each of the batch of beams, and choose the first n
    tokens from that sequence. For details, please refer to docs/speculative_sampling.md.

      beams: (batch_size, beam_width, beam_length)

      log_probs_by_llm: (batch_size, beam_width, beam_length, vocab_size) Log probability
      distribution over the vocabulary for the initial draft token sampled from the vanilla LM head
      of the LLM and each candidate token from the drafter. For each input token, the LLM outputs
      the probability distribution of the next token, so we have
      log_probs_by_llm[:,:,k,:]=P(beams[k+1]).

      log_probs_by_drafter: (batch_size, beam_width, beam_length-1, vocab_size) Log probability
      distribution over the vocabulary for each drafted candidate token in the beams.  The beam
      search algorithm outputs the token distribution for each drafted token, so we have
      log_probs_by_llm[:,:,k,:]=Q(beams[k]).  Please be aware of the shifting of the index k
      compared with log_probs_by_llm.

    Returns:

      n_tokens_in_seq: (batch_size) The number of candidate tokens chosen in the chosen seq_in_beam.

      seq_in_beam: (batch_size) The index of the chosen sequence in each of the batch of beams.

    """
    assert beams.dtype in [mx.int32, mx.int64]

    # Due to the index shift between log_probs_by_llm and _by_drafter, removing the last P(v) from
    # log_probs_by_llm makes it comparable with log_probs_by_drafter.
    log_probs_by_llm = log_probs_by_llm[:, :, :-1, :]

    # Remove the token sampled from the vanilla LM head of the LLM from beams and leaves only those
    # from the drafter model.
    drafted_tokens = beams[..., 1:]

    # Look up Q(drafted_tokens) and P(drafted_tokens) in log_probs_by_drafter and log_probs_by_llm.
    drafted_tokens = drafted_tokens[..., None]
    by_drafter = mx.take_along_axis(log_probs_by_drafter, drafted_tokens, axis=-1).squeeze(axis=-1)
    by_llm = mx.take_along_axis(log_probs_by_llm, drafted_tokens, axis=-1).squeeze(axis=-1)

    # If a drafted token v has P(v) > Q(v), compare(v)==True for sure; otherwise, compare(v)==True
    # is subject to the probability P(v)/Q(v).
    compare = (
        mx.random.uniform(low=0, high=1, shape=by_drafter.shape) < mx.exp(by_llm - by_drafter)
    ).astype(mx.int32)
    # Among candidate token sequences in each of the batch of beams, choose the sequence that has
    # the longest prefix Trues in compare.
    n_tokens_in_seqs = mx.sum(
        mx.cumsum(compare, axis=-1) == mx.arange(1, compare.shape[-1] + 1), axis=-1
    )

    kth = n_tokens_in_seqs.shape[1] - 1
    seq_in_beam = mx.argpartition(n_tokens_in_seqs, kth=kth, axis=-1)[:, kth:]
    n_tokens_in_seq = mx.take_along_axis(n_tokens_in_seqs, seq_in_beam, axis=-1)
    return n_tokens_in_seq.squeeze(axis=-1), seq_in_beam.squeeze(axis=-1)


def _greedy_prepare_next_input(
    beams_by_llm: mx.array,
    last_hidden_state: mx.array,
    seq_in_beam: mx.array,
    n_tokens_in_seq: mx.array,
) -> Tuple[mx.array, mx.array]:
    """Prepare the last_hidden_state and input_ids for the next generation of draft
    candidate tokens when greedy_search is enabled.

      beams_by_llm: (batch_size, beam_width, beam_length)

      last_hidden_state: (batch_size, beam_width, beam_length, hidden_size)

      seq_in_beam: (batch_size) Sequence index with the maximal number of accepted tokens.

      n_tokens_in_seq: (batch_size) The maximal number of accepted tokens per prompt per
      step.

    Returns:

      accepted_last_token_state: (batch_size, hidden_size) Last hidden state of the last
      accepted token in the accepted beam for the next iteration.

      next_tokens: (batch_size) The next token sampled from the current iteration for the
      next iteration.
    """
    # select next tokens
    selected_seqs = _select_one_per_row(beams_by_llm, seq_in_beam)
    next_tokens = _select_one_per_row(selected_seqs, n_tokens_in_seq)

    # select last token state
    selected_last_hidden_state = _select_one_per_row(last_hidden_state, seq_in_beam)
    accepted_last_token_state = _select_one_per_row(selected_last_hidden_state, n_tokens_in_seq)
    return accepted_last_token_state, next_tokens


def _prepare_next_input(
    log_probs_by_drafter: mx.array,
    log_probs_by_llm: mx.array,
    last_hidden_state: mx.array,
    seq_in_beam: mx.array,
    n_tokens_in_seq: mx.array,
) -> Tuple[mx.array, mx.array]:
    """Prepare the last_hidden_state and input_ids for the next generation of draft
    candidate tokens.

      log_probs_by_drafter: (batch_size, beam_width, beam_length-1, vocab_size)
      Draft head log probs for beams.

      log_probs_by_llm: (batch_size, beam_width, beam_length, vocab_size)
      LLM log probs for beams and the predicted next token beyond beam candidates.

      last_hidden_state: (batch_size, beam_width, beam_length, hidden_size)

      seq_in_beam: (batch_size) Sequence index with the maximal number of accepted tokens.

      n_tokens_in_seq: (batch_size) The maximal number of accepted tokens per prompt per
      step.

    Returns:

      accepted_last_token_state: (batch_size, hidden_size) Last hidden state of the last
      accepted token in the accepted beam for the next iteration.

      next_tokens: (batch_size) The next token sampled from the current iteration for the
      next iteration.

    """
    # Select according to the chosen beam index.
    candidate_length = log_probs_by_drafter.shape[-2]
    last_log_probs_by_llm = log_probs_by_llm[:, :, -1, :]
    log_probs_by_llm = log_probs_by_llm[:, :, :-1, :]

    selected_log_probs_by_drafter = _select_one_per_row(log_probs_by_drafter, seq_in_beam)
    selected_log_probs_by_llm = _select_one_per_row(log_probs_by_llm, seq_in_beam)
    selected_last_log_probs_by_llm = _select_one_per_row(last_log_probs_by_llm, seq_in_beam)
    selected_last_hidden_state = _select_one_per_row(last_hidden_state, seq_in_beam)

    # Check if the entire beam is accepted or not.
    entire_beam_accepted = mx.expand_dims(n_tokens_in_seq == candidate_length, axis=-1)
    # If the entire beam is accepted, we use maybe_last_probs to sample next token.
    beam_last_probs = mx.exp(selected_last_log_probs_by_llm)

    # Note the shape of selected_log_probs_by_drafter and selected_log_probs_by_llm is the same
    # as [batch_size, candidate_length, vocab_size].
    # Thus, we clamp resampe_index to be up to candidate_length - 1.
    # Since when n_tokens_in_seq == candidate_length, we use maybe_last_probs above.
    # next_token_index = torch.clamp(n_tokens_in_seq, max=candidate_length - 1)
    next_token_index = n_tokens_in_seq - (n_tokens_in_seq == candidate_length).astype(
        n_tokens_in_seq.dtype
    )
    next_token_log_probs_by_drafter = _select_one_per_row(
        selected_log_probs_by_drafter, next_token_index
    )
    next_token_log_probs_by_llm = _select_one_per_row(selected_log_probs_by_llm, next_token_index)
    # Rejection sampling probs.
    #
    # probs = torch.clamp(
    #    torch.exp(next_token_log_probs_by_llm) - torch.exp(next_token_log_probs_by_drafter),
    #    min=0.0
    # )
    probs = mlx.nn.relu(
        mx.exp(next_token_log_probs_by_llm) - mx.exp(next_token_log_probs_by_drafter)
    )
    probs = mx.where(entire_beam_accepted, beam_last_probs, probs)
    # Modified for mlx: mlx.random.categorical only supports logits while the pytorch version
    # uses probs.
    log_probs = mx.log(probs)
    next_tokens = mx.random.categorical(logits=log_probs)
    # Collect the draft input for next
    accepted_last_token_state = _select_one_per_row(selected_last_hidden_state, n_tokens_in_seq)
    return accepted_last_token_state, next_tokens
